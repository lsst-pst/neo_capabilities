\section{Image Differencing Evaluation}


\subsection{From JPL paper}

Two sources of false detections: random noise and image differencing artifacts. The easiest to model are those arising from random noise in the detectors, which are randomly distributed with an areal density depending on the Signal-to- Noise Ratio (SNR) of the detection threshold.
The other type of false detection arises from artifacts in the image differencing technique that LSST will use to detect moving objects. LSST will build up a deep-stack, fiducial image of the sky, known as a ``template sky'', by combining numerous images of the same field. Individual images are then compared to the template sky and sources that are present only in the single visit are considered potential moving objects. This process readily reveals variable but stationary objects (e.g., variable stars) and these will be effectively removed from the detection stream. The process also generates ``Difference Image Artifacts'' (DIAs) that arise from any number of quirks in the differencing process, e.g., diffraction spikes and imperfect image alignment, to name but two. These can be screened to some extent by machine- learning algorithms, but their areal density in the LSST data stream has not yet been fully characterized. It is important to note that DIAs can be correlated, both within individual images and between pairs of images, and thus their positions can shift between images in a way that can mimic the motion of real solar system objects.
For false detections stemming from random noise we will generate our own lists with an agreed model [7] and collaborators may share or exchange lists to facilitate comparison. For DIAs, we will initially generate lists assuming a random distribution with a range of areal densities to understand the noise loading that causes the linkage engine to falter. The LSST Project is currently using real data, e.g., Dark Energy Camera images, to refine estimates of the rate of DIA creation and understand the effectiveness of DIA rejection techniques. We expect that this effort will provide a more refined model for injecting DIAs, including correlation rules. As our study progresses, we expect that LSST will provide DIA lists for a given OpSim run, which could be directly merged with synthetic and random false detection lists and fed to the linking engine.



JPL paper says: 672 random false positives per focal plane, or 72 / sq. deg. 

\subsection{Intro and Test Data}

False positives have historically dominated over true transients in previous
surveys. \citep{denneau13, goldstein15, kessler15} Major impediment to linking
detections of moving objects.

Our goal in this section is to develop a estimate of the rate of
false DIA source detections by the LSST stack as it exists today. This code
largely derives from the HOTPANTS package \citep{becker15}, as was used for
surveys such as SuperMACHO and ESSENCE (\textbf{XXX: Correct?}). This software
is very functional as-is, but is presently undergoing significant improvement
and refactoring in order to perform at the level required for LSST. In this work
we conservatively assume that the pipeline used for LSST will have the the same
performance as the current code; while we believe that the code will become
significantly better, we can guarantee that it will become no worse.

\textbf{XXX: Skipping sensor comparison, since lots of the Pan-STARRS FP's look like
optical artifacts, not electronics.}

The data we use are a subset of a Decam NEO survey (PI: L. Allen) conducted in
the first half of 2013. These consist of 60 second exposures separated by five
minutes on each field. Due to the difference in telescope aperture, this data is
somewhat shallower than the 30 second visits by LSST. In this work we used five
fields each with between three and five visits, and arbitrarily selected one of
the visits to serve as the ``template'' exposure. In LSST operations, coadded
prior exposures will be used as templates from image differencing rather than
single visits, which will reduce the noise in template images. Our use of single
visits means that some moving objects or transients will appear as negative
sources in the difference images, but we disregard these sources for this
analysis (our goal is to mimic LSST operations rather than discover all possible
transients from the given dataset).

\subsection{Processing}

Some reference to LSST software heritage and version, circa Jan 2016
\citep{juric15}.

\textbf{XXX: introduce term DIA source somewhere.}

Differencing two images, rather than a template vs image. Running force
photometry. Ingest into DB.

\subsection{Correlated Noise}

The resulting DIA Source detections are extremely numerous, far above that
expected from both Gaussian noise and from real transients or moving objects.
Counting only positive detections, there are $\sim 20 000$ detections per
square degree, while the number of expected transients is of order hundreds.

\begin{figure}
  \centering
  \plotone{figures/snr_comparison.pdf}
  \caption{
  Histogram of the reported SNR of sources measured in the difference image
  (left), with the expected SNR as estimated from force photometry on the input
  images (right). The blue line indicates the expected SNR distribution based
  on Gaussian noise. The difference between these two histograms illustrates the
  misestimation of the significance of diaSource detections. Using the correct
  SNR values, the vast majority of these detections are $<5 \sigma$ and can be
  disregarded.
  }
  \label{fig:snr_comparison}
\end{figure}

The source of this excess of detections can be seen in
Figure~\ref{fig:snr_comparison}. When the LSST pipeline convolves the science
image to match the PSF of the template image, that smoothing reduces the per
pixel variance in the image. This reduction is reflected in the variance plane
that accompanies each exposure during processing, which is used for estimating
the significance of detections and the uncertainties on source measurements. A
histogram of the number of sources at a given reported SNR (flux over reported
uncertainty) is show in the left panel of Figure~\ref{fig:snr_comparison}. The
blue line shows the expected counts from Gaussian statistics. This distribution
clearly ramps up at much higher reported SNR values than would be expected.

Because we are only differencing two exposures, rather than an exposure against
a coadded template, we can also estimate the significance of any given detection
from the noise in the input images. For each difference image detection, we also
perform force photometry (photometry with the position fixed at the original
difference image detection) on both the ``science'' and ``template'' images. It
is these same pixel values that go into the measurement on the difference image,
but the force photometry is unaffected by convolution with the matching kernel.

The SNR of detections computed through force photometry is shown in the right
panel of Figure~\ref{fig:snr_comparison}. The bulk of these detections are
clearly at much lower significance, and they largely match the increased number
that one would expect from Gaussian statistics if our detection threshold had
been set closer to $4\sigma$.

The underlying cause of this discrepancy between reported SNR values and the
values computed through force photometry lies in the correlation of pixel values
induced by convolution with the matching kernel. While this reduces the per
pixel variance, because neighboring pixels are now correlated, there is an
excess of random fluctuations on the size scale of the PSF over what would be
expected from uncorrelated Gaussian noise. There is thus noise in the
off-diagonal terms of the covariance matrix, which is not tracked.

Tracking the covariance caused by multiple convolutions is a planned feature for
the LSST stack, but is not currently implemented. Previous surveys such as
Pan-STARRS have used small covariance ``pseudo-matricies'', which tracks the
covariance between a small region of neighboring pixels, then assumes that this
relationship between pixels is constant across an image (P. Price, priv. comm.).
This avoids the creation of the full $N_{\rm pixels}$ by $N_{\rm pixels}$
covariance matrix, which is impractically large and mostly empty.

For this analysis we will use the significance estimates from the force
photometry, as they reflect the correct measurement uncertainties even though
they are not generated in the same way that will be used in LSST production
runs.

\subsection{False-Positive Results}

\begin{deluxetable}{ll}
  \tablecolumns{2}
  \tablecaption{DIA source detection rates. \label{table:fp_rates}}
  \tablewidth{0pt}
    \tablehead{
    \colhead{Source Type}          & \colhead{All visits, counts per sq. deg} \\
    }
    \startdata
    Raw Positive Sources                     & 19,475                         \\
    Raw Negative Sources                     & 23,018                         \\
    Dipoles (not included below)             & 1,609                          \\
    Positive after $5\sigma$ cut             & 1,022                          \\
    Negative after $5\sigma$ cut             & 600                            \\
    Positive sources excluding ``variables'' & 344                            \\
  \enddata
\end{deluxetable}

After removing all detections that did not have $>5\sigma$ significance, the
average number of (positive) sources is $\sim 1000$ per square degree, with some
particular fields having as few as $500$ per square degree. A large fraction of
these detections are the result of stars that have been poorly-subtracted and
left significant residuals in the difference image. It is a common problem for
subtracted stars to exhibit ``ringing'' with both positive and negative
excursions, and these images are no exception. Because the focus of this work
is on detecting moving objects rather than variable stars or transients, we have
not attempted to correct these subtraction artifacts. Instead we exclude
difference image detections which are coincident with significant detections in
both science and template images---that is, we exclude any DIA sources that
overlap with a star. The area lost due to this masking is less than $1\%$ of the
total sky. Again this is not the intended behavior of LSST during production,
but instead a temporary expedient we can use for conservatively estimating the
system's performance.

After excluding the DIA sources associated with stationary objects, the
remaining moving object candidate detections number on average $\sim 350$ per
square degree. As a further estimate on the fraction of these remaining objects
that are false, we visually classified one focal plane of detections either as
obvious imaging artifacts, obvious PSF-like detections, or unidentifiable
detections. Approximately 25\% of the reported detections were clearly some sort
of uncorrected artifact (we did not pursue the cause of individual artifacts),
25\% appeared to be acceptable PSF-like features, and the remaining 50\% were
ambiguous or had too low of signal to noise to be able to classify.


Results from trying to make tracklets, hopefully?? On visual inspection, 25\%
look like junk, 25\% looked like good detections, and the rest were ambiguous
because low SNR. SNR power law exponent is $\sim -2.5$.

Very few detections around bright stars; mostly well-masked by the code. Few
large scale detected artifacts, but not a very big sample of images.

How detailed of a recipe are we providing here; should it be the same level of
detail as we sent to JPL?
