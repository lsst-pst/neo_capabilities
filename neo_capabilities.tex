\documentclass[12pt,preprint]{aastex}
\usepackage{lsst}
\usepackage{xspace}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{comment}

\newcommand{\Alert}{\code{Alert}\xspace}
\newcommand{\Alerts}{\code{Alerts}\xspace}
\newcommand{\DIASource}{\code{DIASource}\xspace}
\newcommand{\DIASources}{\code{DIASources}\xspace}
\newcommand{\DIAObject}{\code{DIAObject}\xspace}
\newcommand{\DIAObjects}{\code{DIAObjects}\xspace}
\newcommand{\DB}{{Level 1 database}\xspace}
\newcommand{\DR}{{Level 2 database}\xspace}
\newcommand{\Object}{\code{Object}\xspace}
\newcommand{\Objects}{\code{Objects}\xspace}
\newcommand{\Source}{\code{Source}\xspace}
\newcommand{\Sources}{\code{Sources}\xspace}
\newcommand{\ForcedSource}{\code{ForcedSource}\xspace}
\newcommand{\ForcedSources}{\code{ForcedSources}\xspace}
\newcommand{\CoaddSource}{\code{CoaddSource}\xspace}
\newcommand{\CoaddSources}{\code{CoaddSources}\xspace}
\newcommand{\SSObject}{\code{SSObject}\xspace}
\newcommand{\SSObjects}{\code{SSObjects}\xspace}
\newcommand{\VOEvent}{\code{VOEvent}\xspace}
\newcommand{\VOEvents}{\code{VOEvents}\xspace}
\newcommand{\transSNR}{5\xspace}

\begin{document}
\title{Large Synoptic Survey Telescope as a Near-Earth Object Discovery Machine}

\author{R. Lynne Jones$^1$, Colin Slater$^1$, Joachim Moeyens$^1$, 
Lori Allen$^2$, Mario Juri\'{c}$^1$,  \and \v{Z}eljko Ivezi\'{c} $^1$}

\affil{
$^1$University of Washington, \\
$^2$National Optical Astronomy Observatory}


\begin{abstract}
We discuss the ability of LSST to contribute to Near-Earth Objects (NEO) discoveries and
Congressional Brown mandate to NASA. The two main issues addressed here are robustness 
of the LSST strategy for discovering NEOs using nightly pairs of observations, and the 
expected cumulative completeness for potentially hazardous asteroids (PHAs) with 
visual absolute magnitudes $H<22$.  We argue that the observing and data processing 
strategies chosen by LSST are robust, and would yield a completeness of about 70\% with 
the current LSST baseline survey. We describe a number of modifications of the LSST baseline 
survey which could potentially yield the completeness of 90\% for $H<22$ PHAs. 
\end{abstract}

\keywords{}

\section{Introduction}

XXX This is so totally work in progress. ZI push-ed it just as a backup... XXX


Main parts:
\begin{itemize}
\item NEO impacts as concern; the Brown mandate, NASA panels,
  introduce LSST 
\item LSST defaults and claimed performance in older publications
\item informal concerns by the community, GMS paper
\item study by the JPL team, technical support from LSST (assuming
  baseline cadence)
\item exploration of baseline cadence modifications designed to boost
    NEO completeness 
\item Outline of this paper
\end{itemize} 


The small-body populations in the Solar System, such as asteroids, trans-Neptunian objects (TNOs) 
and comets, are remnants of its early assembly. Collisions in the main asteroid belt between Mars and 
Jupiter still occur, and occasionally eject objects on orbits that may place them on a collision course 
with Earth. About 20\% of this near-Earth Object (NEO) population, the so-called potentially hazardous 
asteroids (PHAs), are in orbits that pass sufficiently close to Earth's orbit, to within 0.05 AU, that 
perturbations with time scales of a century can lead to intersections and the possibility of collision. 
In December 2005, the U.S. Congress directed\footnote{For details see http://neo.jpl.nasa.gov/neo/report2007.html} 
NASA to implement a NEO survey that would catalog 90\% of NEOs with diameters larger than 140 meters 
by 2020 (the George E. Brown, Jr. mandate). For a compendium of information about NEOs and PHAs 
and an up-to-date summary of discovery progress, see NASA's NEO webpage\footnote{http://neo.jpl.nasa.gov/neo/}. 

The completeness level set by Congressional mandate can be fulfilled with a 10-meter-class ground-based
telescope equipped with a multi-gigapixel camera, and a sophisticated and robust data processing system. 
The Large Synoptic Survey Telescope (LSST), currently being constructed, is such a system (for a concise
system description, science drivers and other information, see \citep{LSSToverview}). Early simulations of 
LSST performance presented by \cite{IvezicNEO2007} showed that the 10-year baseline cadence would 
result in 75\% completeness for PHAs greater than 140 m (more precisely, for PHAs with $H<22$; see 
\S~XX for further discussion). They also suggested that with additional optimization of the observing cadence, 
LSST could achieve 90\% completeness. Such optimization was discussed by \cite{LSSToverview} who
reported that, to reach 90\% completeness, about 15\% of observing time would have to be dedicated to NEOs
and the survey would have to run for 12 years.  
%% From the overview paper: 
%% - the LSST baseline cadence provides orbits for 82% of PHAs larger than 140 meters after 10 years of operations
%% - 84% completeness with minor changes to the cadence (5% of time for NEO-optimized observations)
%% - 90% completeness with major changes to the cadence (15% of time for NEO-optimized observations and 12 years)
The latest LSST simulation results, presented in \cite{JJI2016}, yielded a completeness of $\sim$72\% for
PHAs with $H<22$ using the current 10-year baseline survey. The minor difference compared to older
studies is attributable to the differences in simulated NEO populations and other modeling details. 


\cite{JPLstudy} described a new study, related to this work. Their preliminary results indicate a completeness
of $\sim$65\% for NEOs with $H<22$. The difference compared to \cite{JJI2016} result (72\%) is due 
to XXX (true?): PHA vs. NEO, possible slope of the size distribution. 


We're going to evaluate moving object detection capabilities with
LSST, comparing performance of our current prototype pipelines with 
the required performance during operations.  The two main goals are
to demonstrate that i) MOPS can cope with false detection in image differences,
and that ii) the NEO detection performance of the LSST baseline cadence can be
further boosted by adequate modifications 


From JPL paper -- which gives a concise summary of the main simulation aspects. 

The LSST baseline survey cadence relies primarily on single night pairs of detections, 
with roughly 30 minutes between the elements of a detection pair. These pairs form 
what are known in MOPS parlance as tracklets, and sets of tracklets are linked across 
multiple nights to form tracks, which can then be sent to the final step, which is orbit 
determination. The strategy of using pairs is an aggressive and potentially fragile
approach, but theoretically represents the most productive NEO search with the minimum 
impact on other LSST science drivers. An alternative to visit each field three times per 
night to form tracklets from triplets of detections may prove more robust, but likely 
with a penalty of reduced performance. One of our study objectives is to understand the
tradeoffs between these two approaches.

The two major questions to be addressed by our study can be informally stated as 
``Will MOPS work?'' and ``If MOPS works what fraction of NEOs will LSST discover?''. 

Main problems:
\begin{enumerate}
\item Linking large number of detections in the presence of false positives (false detections due to problems 
with image differencing software). 
\item Adequacy of data, including image depth, sky coverage and cadence, to reach the required 
completeness level. 
\end{enumerate} 

Therefore, the main analysis components to check are: 
\begin{enumerate}
\item The performance of image differencing, with emphasis on the rate and properties of 
   false detections 
\item Linking large number of detections in the presence of false positives 
\item Observing cadence simulations coupled with NEO population models to forecast 
        discovery rates 
\end{enumerate} 



\cite[hereafter GMS]{GMS2016} reported different NEO completeness levels than
published by the LSST team in 2007 and 2014 . There are three main 
reasons why the GMS results differ:
\begin{enumerate}
\item GMS used a different realization of the LSST baseline survey
\item GMS used different synthetic NEO populations to evaluate completeness
\item GMS {\it redefined} the completeness limit from the commonly
  used $H<22$ criterion to an albedo-dependent value of $H$ limit (which
  attempts to directly model the $D>140$ m size cut)
\end{enumerate}

Regarding the last point, GMS found that the completeness drops by 5\%
when $H<22$ criterion is replaced by $D>140$ m criterion. Regarding 
the first point, GMS results can be more meaningfully compared to an LSST
study by \cite{JJI2016}, who used the same simulated cadence. After accounting for 
the $H<22$ vs. $D>140$ m methodological difference of 5\%, GMS obtained a 
completeness of 67\% using 3 pairs in 12 nights (for simulated cadence {\it enigma\_1189}), while Jones et al. 
study obtained $\sim$73\% using 3 pairs in 15 nights (for simulated cadence {\it minion\_1016}, which is 
statistically very similar to {\it enigma\_1189}). This difference 
of $\sim$5\% is attributable to the differences in simulated NEO
populations and other modeling details. In summary, GMS find the NEO
completeness in the range $\sim$60\% to $\sim$70\% for the LSST
baseline cadence. The variation is due to different NEO populations,
different NEO detection criteria, and other specifics. When accounting 
for different choices of simulation parameters, their results are
consistent with the results published by the LSST team. 

But: how much higher can the completeness be pushed with cadence
modifications optimized for NEOs? 

In Observing Strategy white paper: fig. 3.4 gives 73.4\% for PHAs with 
$H<22$ and {\it minion\_1016}, using 3 pairs in 15 nights. 


\begin{enumerate} 
\item Control and quantify the rate of (false positive) detections
\item Software (and computational capacity) capable of inter-night linking of detections given the expected rates
\item Quantify the discovery yields (and their robustness) under those assumptions
\end{enumerate} 


The leading systematic effects in completeness estimates are: 
\begin{enumerate}
\item NEO vs. PHA difference (the completeness is about $\sim$5\% higher for PHAs than for NEOs) 
\item Different sample definitions: $H<22$ vs. $D>140$m (as shown by \citep{GMS2016}, completeness
           increases by $\sim$5\% when $H$-based criterion is used) 
\item Orbital parameter distribution for the simulated asteroid population (e.g. the Bottke model
             vs. the Granvik model; varying populations contribute completeness uncertainty of a few percent) 
\item Variations of ``discovery window'' (e.g., X visit pairs in N nights: changing N from 15 to 30 with X=3 increases
          completeness by 3\%; changing X from 3 to 4 with N=15 decreases completeness by 6\%). 
\item Variations of the nominal detection threshold (if the detection threshold is changed from the 
          signal-to-noise ratio of 5 or greater to 4 or greater, the completeness is boosted by 2-3\%; 
          the difference between the optimal detection using trailed profile and point-spread-function 
          detection, which is negligible for LSST baseline exposure time of 30 seconds, would be worth 2\%
          in completeness for doubled exposure time). 
\item Sensitivity to details in sky coverage and cadence (e.g. nightly pairs of visits vs. quads of visits;
          requiring quads instead of pairs of visits decreases completeness by 30\% using baseline cadence; 
          about half of that loss can be recovered using cadence simulations that request four visits per night) 
\item Uncertainties when predicting effective image depth (system throughput, variation of the detection efficiency
          with the signal-to-noise ratio, treatment of trailing losses); for a survey that has a completeness above 60\%, 
          each additional one magnitude of depth for a given survey cadence increases the completeness by another 10\%.
\item Uncertainties when predicting apparent flux (albedo distribution, phase effects, photometric variability 
          due to non-spherical shapes, color distributions); assuming an uncertainty of 0.2 mag in the effective 
          limiting magnitude, the corresponding  systematic uncertainty in completeness is about 2\%.)
\item The slope of the asteroid size distribution (current measurement uncertainty of this parameter 
          corresponds to a systematic uncertainty in completeness of about 2\%.)
\item The impact of known objects (assuming that 43\% objects would be discovered by the start of
          LSST survey, \citep{GMS2016} boosted the final PHA completeness for LSST baseline survey by 11\%). 
\end{enumerate} 

Given these systematic effects, a comparison of different simulation results (both for the same system,
and those of different systems, especially systems operating at different wavelengths) has to be undertaken
with due care. It is unlikely that a meaningful quantitative comparison can be pushed beyond a level
of a few percent (and perhaps as much as 10\%). In practice, the completeness of a given operating survey
is best estimated using the object re-discovery rate. 


From Mario's talk to NASA:

LSST will detect variability (motion and flux variability) by
differencing each incoming image against a deep template.
Sources will be detected at an S/N=5 threshold (see Appendix A). 

We expect on average about 1,000 per sq. deg. astrophysical, real,
detections, including up to about 500 asteroids per sq. deg on the 
Ecliptic (for scale, the LSST field of view is about 10 sq. deg., with 
about 20 4kx4k CCDs per sq. deg.)

We also expect a false-positive detections due to random
fluctuations in the background at a level of about 200 per
sq. deg. (all at the faint end).  XXX check Colin's report 

However, historically surveys have reported factors of 10 to 500 times
more (depending on the survey; see \citep{denneau13};
\citep{goldstein15} ). 
Those additional false positive
detections are due to systematic effects: 
\begin{itemize} 
\item Camera and telescope artifacts
\item Imperfect image subtractions
\item Cosmic rays
\end{itemize} 

For a ``menagerie'' of artifacts (with amusing names such as 
{\it chocolate chip cookies, frisbee, piano, arrowhead, UFO}), from
Pan-STARRS, see Fig.~17 in \cite{denneau13}. 


``Many of the false detections are easily explained as internal
reflections, ghosts, or other well-understood image artifacts,...''


Learning from PS1 Experience: PanSTARRS was a first generation
experiment. Over the past decade, subsequent surveys (including LSST) 
have learned tremendously from the PS1 experience. There are surveys 
running today which have largely solved the key problems that PS1 has encountered.
These are recent developments driven largely by extragalactic and
transient science cases. They are not yet well known beyond those
communities and reporting on those developments is additional
motivation for this paper. 
 
Major improvements to hardware include CCDs with significantly fewer 
artifacts (e.g. DECam, see below; LSST) and optical systems designed to
minimize ghosting and internal reflections (e.g. LSST). 

Improvements to the software include advanced image differencing
pipelines (e.g., PTFIDE for the Palomar Transient Factory and the
Zwicky Transient Facility) and various machine learning classifiers
for filtering false positives (see below). 

DECam: \cite{goldstein15} 

The Dark Energy Survey (DES) is an optical/near-infrared survey that
aims to probe the dynamics of the expansion of the universe and the
growth of large scale structure by imaging 5,000 sq. deg. of the
southern sky. It is technologically very similar to LSST with
\begin{itemize}
\item 520 Mpix camera, 62 mosaicked chips (deep depleted devices)
\item 3 sq.deg. field of view, same filter bands as LSST
\item Single-exposure depths comparable to LSST
\item Includes a supernova search program which employs image
differencing methods analogous to LSST’s  and detects objects at the 
same effective signal-to-noise ratio as LSST (S/N=5)
\end{itemize} 

The false positives in DECam data are morphologically much simpler
(compare Fig.~1 in \citep{goldstein15} to Fig.~17 in \citep{denneau13})
than those in Pan-STARRS, and thus are much more amenable to automated 
screening using machine learning methods. Using a Random Forest 
classifier, \cite{goldstein15} cleaned their sample from having a 
raw false detection rate of 13:1 to a filtered rate of 1:3. This performance
is already within the acceptable range for LSST performance goals. 


{\bf Need to refer to section by Colin.} 

LSST will use two methods to detect moving objects
\begin{enumerate}
\item Detecting trailed motion on the sky:  objects trailed by more
  than 2 PSF widths (corresponding to motion faster than about 1
  deg/day) will be easily detectable as trailed.  Two trailed
  detections within 30--60 minutes in a single night will be
  sufficient to identify an object as an NEO candidate,
\item Inter-night linking of pairs: this technique will recover
  objects moving too slow enough to be measurably elongated in 
  a single exposure. 
\end{enumerate} 

MOPS: Given the expected false-positive rates demonstrated by
\cite{goldstein15} and in Colin's  section, LSST MOPS linking will
be possible. This has already been shown by the PanSTARRS project 
with simulations performed for the PS4 system\footnote{PanSTARRS 
PS1 experience does not contradict this conclusion. In addition to 
hardware issues, PS1 was only 1/4 of the assumed system (see 
\citep{denneau13} for more details). }, which is in this
respect equivalent to LSST. The robustness to unexpected false
positives is further tested with simulations performed by LSST,
as described below. 







\section{LSST Solar System Survey} 

Opening paragraph  (lift text from overview and Lynne's IAU paper) 

Possible subsections here or below:

- Concepts for discovering moving objects

- Outline for simulations (see Chesley) 




\subsection{Brief Overview of LSST} 
%\include{LSSToverview} 

\subsection{LSST Observing Strategy} 

XXX get text from the overview paper and Science Book 

As deployed and funded (by the U.S National Science Foundation and
Department of Energy), LSST is primarily a science-driven mission. 
Existing cadence is optimized to maximize the overall science returns
(incl. Solar System science), rather than NEO/PHA discovery
completeness (though the two goals are highly interrelated).  As designed, the survey is not optimized for rapid
discovery and follow-up of all types of moving objects\footnote{
XXX What's the purpose of this footnote? Note that LSST will enable rapid identification and follow-up of
trailed objects (within 60 seconds of discovery). If deployed with a 
planetary-defense optimized cadence, the NEO yields could be
significantly improved, and approaching the 90\% completeness level
for $H<22$.} 
Early simulations indicate 90\% is achievable for NEO-optimized
cadence. However, other science goals would be affected (including
Solar System science!).  XXX Refer to Jones et al.  (2016) 

The current baseline cadence is optimized for science returns.
It is expected to yield approximately $\sim$70\% of the extant NEO population.


\subsection{LSST  Data Management and Image Processing} 

Refer to \cite{DM2016} and Appendix A. 


%\section{Image Differencing Performance}
\include{image_differencing}

%Paper on ``Automated Transient Identification in the Dark Energy Survey''  is \cite{goldstein15}. 


\section{Moving Object Processing Pipeline Evaluation}

XXX see below for basic scalings 

Zeljko and Mario. There is a report on MOPS (LDM-156)...   XXX refer to its
results here... 

Quoting \cite{denneau13}: ``MOPS achieves $>$99:5\% efficiency in
producing orbits from a synthetic
but realistic population of asteroids whose measurements were
simulated for a Pan-STARRS4-class telescope. \dots MOPS has been
adapted successfully to the prototype Pan-STARRS1 telescope despite
differences in expected false detection rates, fill-factor loss, and
relatively sparse observing cadence compared to a hypothetical
Pan-STARRS4 telescope and survey.'' 

But we did our own analysis, too...

The LSST project has developed an enhanced prototype implementation of MOPS.
We ran simulations with LSST system and cadence, and a significantly
wider range of false positive candidate rates. 

Known limitations/caveats:
\begin{itemize}
\item Due to computational constraints at the time when the simulation
  was performed (2011), a $v < 0.5$ deg/day velocity limit was
  imposed.
\item For similar reasons, the filters were imposed on track fitting
  were not optimized, artificially reducing the yield
\item As we understand the algorithmic scalings, these will not change the
final results; nevertheless they are being actively mitigated by new
simulations which are in progress.
\end{itemize}

Simulation results: Asteroids are discoverable in the presence of significant noise.
Get more details from Lynne \& Axelrod writeup. 

\newpage

\subsection{The Basic Strategy} 

First form tracklets, and then candidate tracks. Don't worry about false positives, 
be it detections, tracklets or tracks, because the initial orbit determination (IOD) 
will efficiently and reliably filter out false tracks (due to high-accuracy astrometry 
and well-understood simple Keplerian model predictions). The essential question
is whether the resulting numbers of false tracklets and tracks can be handled with 
available computing resources.

Assuming 0.005 sec per IOD (based on an analysis by Pan-STARRS MOPS team,
reference XXX; possibly shorter now), 1000 cores, and 12 hours of computation time 
per day, one can filter about 10$^{10}$ candidate tracks per day. How many tracks 
do we expect? 

MOPS tests show that with of the order a million tracklets per night (similar to 
the expected rate, as demonstrated below), a 30-day search window results in 
about the same number of tracks as input tracklets. Assuming 2 million tracklets
per night, a 30-day search window would result in about 60 million candidate
tracks. This expectation is more than two orders of magnitude smaller than the 
assumed system capacity above. As discussed below, the number of false tracks
could be decreased by another order of magnitude by decreasing the mean revisit
time to 10 minutes. 


\subsection{Expected False Tracklet Rates} 


Given a detection in the difference image, we search for a matching detection in another
difference image to form a tracklet. For orientation, the highest sky density of asteroids 
down to LSST faint flux limit ($r \sim 24.5$) is of the order $\rho_{ast} \sim 100$ deg$^{-2}$
(perhaps 2-3 times larger, depending on model assumptions). The number of false positives 
due to (gaussian) background fluctuations, assuming typical LSST seeing (0.8 arcsec) and
SNR$>$5, is about $\rho_{bkgd} = 60$ deg$^{-2}$. However, analysis of DECam images reduced 
using prototype LSST software shows a higher rate of detections in difference images, that 
cannot be readily associated with moving objects. This analysis implies a conservative upper
limit for the false positive rate of about $\rho_{FP} =  400$ detections deg$^{-2}$. This value 
is an upper limit because the analyzed fields are close to the Ecliptic, with a significant but
not well known contribution from real asteroids (due to very faint flux levels, $r \sim 24$). 
Hence, it is possible that the false positive rate is actually as much as four times lower. 

We will assume that the sky density of detections in difference images is given by 
$\rho_{det} = \rho_{ast} + \rho_{FP}$. When searching for a matching detection in another
difference image, there are two distinct types of behavior. Correct matches of detections
of the same asteroids into tracklets follow the behavior expected for correlated samples:
as long as the object's angular displacement between the two epochs is sufficiently larger 
than the seeing disk, while at the same time smaller than the search radius, the number
of matches is simply 
\begin{equation}
             N_{tracklet}^{true} = \rho_{ast}  \, A_{FOV},
\end{equation}
where $A_{FOV}$ is the field-of-view area (for LSST, $A_{FOV}=9.6$ deg$^2$). With 
$\rho_{ast} = 100$ deg$^{-2}$, $N_{tracklet}^{true} \sim 1,000$ per a pair of visits, and with
500 visit pairs per typical observing night, $N_{tracklet}^{true} \sim 500,000$ per night
(the implied number of asteroid detections per night is about a million, but note that 
the sky density of asteroids falls rapidly with the distance from the Ecliptic). We emphasize 
that this number of true tracklets does not directly depend on the search radius, nor the 
time elapsed between the two visits, as long as they have their plausible values (about an 
arcminute, and a few tens of minutes, as discussed further below). 

There are three other types of tracklets that follow behavior for uncorrelated (random) 
samples: associations of different asteroids, associations of asteroids and false detections, 
and tracklets made of two false detections. Assuming the same $\rho_{det}$ in both 
difference images, for each of $N_{det} = \rho_{det} \, A_{FOV}$ detections in one image,
we search for a matching detection in another image. The search radius is given by 
$\delta_{max} = v_{max} \, \Delta t$. Here $v_{max}$ is the  cutoff velocity and $\Delta t$ 
is the time elapsed between the two images. For LSST baseline cadence, $\Delta t$ is in 
the range 20-60 minutes. The search area, $A_S = \pi \delta_{max}^2$, is then 
\begin{equation}
     A_S = 0.0055 \left( v_{max}  \over {\rm deg \, day}^{-1} \right) \, \left(\Delta t \over {\rm hour} \right) {\rm deg}^2.
\end{equation}
Adopting $v_{max} = 1$ deg day$^{-1}$, which ensures a high completeness level even for fast-moving 
NEOs\footnote{Simulations imply that 95\% of NEO detections have $v<1$ deg day$^{-1}$; the completeness
for main-belt asteroids is essentially 100\%. In addition, objects moving faster than 1 deg day$^{-1}$ will
be resolved in LSST images and can be treated separately.}, and $\Delta t = 30$ minutes (which together 
imply $\delta_{max} = 1.3$ arcmin), gives a search area of $A_S = 0.0014$ deg$^2$. 

As long as $\rho_{det}$ is much smaller than $\rho_A = 1/A_S = 733$ deg$^{-2}$, the expected number of 
matches within the search radius is less than unity (for a discussion of second-order effects, see Appendix B 
in \citealt{IVLZ2005}). In this regime, the probability of forming a tracklet is 
\begin{equation}
                 p_{tracklet}^{false} =   { \rho_{det}  \over \rho_A}, 
\end{equation}
and the total expected number of {\it false} tracklets is 
\begin{equation}
           N_{tracklet}^{false} = N_{det} \, p_{tracklet}^{false} =  \rho^2_{FP}  \, A_S \, A_{FOV} \,
                                \left(1 + 2 \eta + \eta^2\right),
\end{equation}
where $\eta = \rho_{ast}  / \rho_{FP}$. With $\rho_{ast} = 100$ deg$^{-2}$ and  $\rho_{FP} = 400$ deg$^{-2}$,
$N_{tracklet}^{false} \sim 3,400$ per pair of visits, and $N_{tracklet}^{false} \sim 1.7$ million per observing night. 
Note that there are false tracklets even when $\rho_{FP} = 0$ because of incorrect associations of different asteroids:
this term contributes 0.1 million tracklets per observing night for $\rho_{ast} = 100$ deg$^{-2}$. The total
number of tracklets is thus to the first order ($\eta \approx 0$)
\begin{equation}
   N_{tracklet} =  N_{tracklet}^{true} + N_{tracklet}^{false} = \rho_{ast}  \, A_{FOV} + \rho^2_{FP}  \, A_S \, A_{FOV}. 
\end{equation}
Therefore, there will be up to about $2\times10^6$ tracklets per observing night, for the chosen parameter 
values. Given that these choices are rather conservative, this estimate is essentially an upper limit;
approximately, {\it we expect of the order a million tracklets per observing night}. 


In addition to $N_{tracklet}^{false}$ scaling with the square of $\rho_{FP}$, $N_{tracklet}^{false}$ scales with the squares of
both $v_{max}$ and  $\Delta t$ (via the dependence on $A_S$). Therefore, if $\Delta t$ would be made
as small as 10 minutes by modifying observing strategy, the resulting $N_{tracklet}^{false}$ would be about an 
order of magnitude smaller (and $N_{tracklet}$ about three times smaller).  Hence, the shortening of $\Delta t$ is 
a good mitigation strategy against high false positive detection rates in difference images\footnote{An
extreme example of this mitigation strategy would be to obtain two consecutive 30-second visits separated 
by 34 seconds (additional 2 seconds due to shutter motion and another 2 seconds due to readout). 
The acceptable false positive density would be increased by about three orders of magnitude, with the
lower limit for the detectable motion of the order 0.1 deg day$^{-1}$.}.


\subsubsection{Can MOPS handle a million tracklets per night?} 

How many tracks can we form with $\sim$10$^6$ tracklets per night, and a 30-day search window? 
Based on MOPS test runs, we should expect well below 10$^8$ candidate tracks. LSST's 1000-core
system will be capable of easily handling about 10$^{10}$ IODs per day, so we have a margin
of about two orders of magnitude. Hence, {\it even if the false positive rate in difference images is 
ten times higher than expected, it can still be handled without a change of baseline cadence.}
And yet another factor of a few increase in the false positive rate can be mitigated by a simple
shortening of the revisit time by about a factor of 3. At the same time, an order of magnitude 
larger false positive rates for LSST than measured using DECam images and prototype LSST software
are rather implausible. If the LSST camera, or other system component, would somehow
cause such high false positive rates, the whole LSST mission would indeed be a failure. 



\newpage
\section{LSST Observing Cadence Optimization}

Lynne, new OpSim runs. 


\section{Conclusions}

{\bf Summary:} Well behaved image differencing and detection are needed for the
asteroid detection strategy adopted by LSST. This has historically
been difficult to achieve. First generation surveys have encountered
significant problems including CCD artifacts, optical system
artifacts, and software issues. Significant progress has been made
since. Contemporary surveys comparable to LSST (specifically, DES) are
already achieving false positive rates below the few:1 ratio required
for LSST MOPS (see next section). LSST image differencing will {\bf
not} be a limiting factor in its ability to discover asteroids
(including NEOs). DES experience has already demonstrated algorithms 
sufficient to meet LSST MOPS requirements.

\begin{enumerate}
\item LSST will employ the 2+2+2 MOPS discovery strategy, and detect fast-moving asteroids via trailing.
\item Existing surveys demonstrate that false positive rates required
  by LSST (approaching $\sim$1:1) are  achievable (Dark Energy Survey;  \citep{goldstein2015}).
\item PanSTARRS PS4 simulations \citep{denneau13}) as well as LSST
  simulations (Myers et al. 2011) demonstrate the ability of MOPS to perform the linkages under those conditions.
\item LSST software and observing strategy are robust to perturbations
  around assumed efficiencies; even large differences in expectation
  cause only a few percent difference in efficiency. 
\item Our simulations indicate that, if the cadence is optimized for
          NEO searches, LSST likely has the capability and capacity to reach the Brown mandate.
\end{enumerate}

Add words about consistency with \cite{GMS2016}. 

\appendix
%\section{LSST Image Processing Steps and Data Products Relevant for Asteroids} \label{sec:AppA}
\include{appendix1} 




\bibliography{neo_capabilities}
\end{document}


To Do:

- ask Lori Allen to be coauthor (and if there are others from her team) 
- ask Tim Axelrod and Jonathan Myers (others from old days?) 

Lynne's new NEO runs: cumulative completeness for H<22 objects: 
Run                              3p/30  |3p/15|    SNR3   SNR0   singlePair  singleDet     Modification
astro_lsst_01_1014       76.1      72.6        79.1    94.4       88.8           92.0        12 deg Ecliptic + WFD, 60sec
astro_lsst_01_1015       79.2      74.8        81.7    96.4       90.3           93.0        15 deg Ecliptic + WFD, 60sec
astro_lsst_01_1016       80.0      77.5        83.1    95.6       88.9           92.1        NEO baseline
astro_lsst_01_1017       80.8      77.8        83.3    95.3       90.1           93.1        EB + 30s, gri
lucy_1001                      76.5      73.7        79.9    95.3       88.8           92.1        15 deg Ecliptic (ri, 60 sec), NES gz  




astro_lsst_01_1015:
- the best (but only about half of WFD visits due to 60 sec exposures) 
- how about 20 deg band around the Ecliptic? Going from 12 to 15 has a
    big impact.
- 3 pairs in 30 days at SNR=4 should be similar in completeness to 3
    pairs in 15 days at SNR=3, but more doable 


Run                              3p/30  |3p/15|    SNR3   SNR0   singlePair  singleDet     Modification
astro_lsst_01_1015       79.2      74.8        81.7    96.4       90.3           93.0        15 deg Ecliptic + WFD, 60sec

- at the bright end (H<20), C for single detection and single pair
   saturates at ~99.4%; this shows that 12-year survey is long enough
   to get even those objects that can ``hide behind the Sun'' 
- 3 pairs in 15 nights with SNR=0 results in C=96.4%; this shows that 
   the impact of finite cadence on completeness for *large* objects is 
   only 3.0%
- 3 pairs in 15 nights with SNR=5 gives C=75%, which compared to 
   a single detection with SNR=5 and C=93% represents a loss of 18%;
   therefore, the impact of cadence on *small* objects is much larger 
   than for large objects; going from a single detection to 3 pairs in
   15 nights is equivalent to losing 2 mag of depth! 
- 3 pairs in 15 nights with SNR=5 crosses C=90% at H~20.5; data 
   deeper by 1.5 mag would bring H<22 completeness to 90% (or we'd
   need to go to SNR=1.3); but to go deeper by 1.5 mag, the exposure 
   time needs to be 16 times longer!! 
- going from 15 nights to 30 nights boosts C by 4.4%; this is
   equivalent to about 0.3 mag deeper data and 1.2 mag of ``missing''
   depth; in this case, the exposure time would have to be ``only'' 9
   times longer to reach C=90% for H<22. 
   
So, reaching C=90% is becoming more and more hopeless. My last 
desperate traces of hope are: 
- we could look at the median brightness of (say) H=22 objects vs.
   ecliptic latitude (and perhaps solar elongation to account for
   sweet spots) and adjust exposure time as a function of latitude 
   (we know that the faintest H=22 detections are close to the
   Ecliptic)
- if the required band width is much less than 15 deg, we would 
   perhaps have enough observing time budget to really go with 5-10
   minutes long exposures. Of course, we'd have to keep them down
   to 1 min or so and shift-and-coadd multiple exposures for a grid
   of possible motions, which would be a major impact on DM 
- we could look at H~22 objects which we missed with 3 pairs in 15
   nights, but which we did *not* miss with ``single detection'' and
   ask what were their SNRs when they were in the FOV but didn't 
   have SNR>5. And where they were on the sky. This sounds like 
   something for Joachim to look at. 
- NASA says ``we'd be happy with even C=80%"  :) 


- 75% of the PHAs: Ivezic & Jones, 2014 (AAS)


TODO from May 29 reading: 
- ref to Chesley & JPL, find his simulations plan
- in Introduction, add info about counts of transients from my AAS
time-domain talk, also refer to Ridgway paper
- 



10 km/s at 100 Lunar Distances is:
384,000 km * ang. speed (radian/s)  = 10 km/s 
ang. speed = 5.4 arcsec / sec  = 5.4 * 3600 * 24 arcsec / day   = 5.4*24 ~ 100 deg/day 
5.4 arcsec/sec * 30 sec ~ 3 arcmin trail! 


The impact of trailing: for 0.8 arcsec seeing and 30 sec exp (tau=1) 
slow: 0.25 deg/day, fast: 1 deg/day

tau    dmT     dmTotSlow   dmTotFast    dmSNRSlow   dmSNRFast 
1.0    0.00         0.03              0.38               0.04              0.25 
2.0    0.38         0.25              1.26               0.10              0.48   
5.0    0.87         0.52              1.78               0.31              0.88

Optimal (trailed) vs. PSF detection for v=1 deg/day is worth 0.13 mag
for nominal exposures, and 0.23 mag for 60 sec exposures (0.15 mag
for v=0.25 deg/day). Adopting 0.2 mag for doubling exposure time, 
this corresponds to about 0.02 in completeness. 